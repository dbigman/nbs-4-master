{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning II: Introduction to Supervised Classification Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## intro image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://miro.medium.com/v2/resize:fit:1400/1*GxoJmn7FPspmr1Gd-sDDyg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning Concepts: Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nearest neighbors is a member of the instance based learning and lazy Learning families. Instance based models base the model on the evaluation of a function that depends on the point we are querying and training data. Nearest Neighbors is the **simplest** of these techniques. The rationale behind this model is as follows: Each training data set can be seen as a solved case/problem. Thus, given a new problem instance we may retrieve the most *similar* case in our data set and apply the same solution. In the case of classification, this means that we select the label of the most similar data example in our training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Let's see what the boundary looks like in a toy problem.\n",
    "\n",
    "MAXN=100\n",
    "X = np.concatenate([1.25*np.random.randn(MAXN,2),5+1.5*np.random.randn(MAXN,2)]) \n",
    "X = np.concatenate([X,[8,5]+1.5*np.random.randn(MAXN,2)])\n",
    "y = np.concatenate([np.ones((MAXN,1)),-np.ones((MAXN,1))])\n",
    "y = np.concatenate([y,np.ones((MAXN,1))])\n",
    "idxplus = y==1\n",
    "idxminus = y==-1\n",
    "plt.scatter(X[idxplus.ravel(),0],X[idxplus.ravel(),1],color='r')\n",
    "plt.scatter(X[idxminus.ravel(),0],X[idxminus.ravel(),1],color='b')\n",
    "\n",
    "from sklearn import neighbors\n",
    "from sklearn import metrics\n",
    "\n",
    "delta = 0.05\n",
    "xx = np.arange(-5.0, 15.0, delta)\n",
    "yy = np.arange(-5.0, 15.0, delta)\n",
    "XX, YY = np.meshgrid(xx, yy)\n",
    "Xf = XX.flatten()\n",
    "Yf = YY.flatten()\n",
    "sz=XX.shape\n",
    "data = np.c_[Xf[:,np.newaxis],Yf[:,np.newaxis]];\n",
    "\n",
    "#Evaluate the model for a given weight\n",
    "clf = neighbors.KNeighborsClassifier(11)\n",
    "clf.fit(X,y.ravel())\n",
    "Z=clf.predict(data)\n",
    "Z.shape=sz\n",
    "\n",
    "plt.imshow(Z, interpolation='bilinear', origin='lower', extent=(-5,15,-5,15),alpha=0.3, vmin=-1, vmax=1)\n",
    "plt.contour(XX,YY,Z,[0])\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(9,9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "\n",
    "+ The boundary is piece-wise linear. It is composed of edges of the Voronoi diagram.\n",
    "+ Observe that the classifier perfectly fits the training data. Adding or removing one data point can largely change the boundary. This implies that the complexity of the method is large.\n",
    "+ The key component of the nearest neigbors classifier is the notion of similarity/distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that regularization explicitly models complexity. Regularization is usually a penalty term. In nearest neighbors we can penalize solutions with small \"support\" by using a majority voting on the $k$ closests data samples to the query sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's see what the boundary looks like in a toy problem.\n",
    "\n",
    "plt.scatter(X[idxplus.ravel(),0],X[idxplus.ravel(),1],color='r')\n",
    "plt.scatter(X[idxminus.ravel(),0],X[idxminus.ravel(),1],color='b')\n",
    "\n",
    "clf = neighbors.KNeighborsClassifier(3)\n",
    "clf.fit(X,y.ravel())\n",
    "Z2=clf.predict(data)\n",
    "Z2.shape=sz\n",
    "\n",
    "plt.imshow(Z, interpolation='bilinear', origin='lower', extent=(-5,15,-5,15),alpha=0.4, vmin=-1, vmax=1)\n",
    "plt.imshow(Z2, interpolation='bilinear', origin='lower', extent=(-5,15,-5,15),alpha=0.2, vmin=-1, vmax=1)\n",
    "\n",
    "plt.contour(XX,YY,Z2,[0])\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(9,9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix (churn classification with nearest neighbor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets analyse a problem of customer churn prediction. We may fit a 1-Nearest Neighbor classifier and check the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare the data\n",
    "churn_df = pd.read_csv('churn.csv')\n",
    "col_names = churn_df.columns.tolist()\n",
    "\n",
    "# Isolate target data\n",
    "churn_result = churn_df['Churn?']\n",
    "y = np.where(churn_result == 'True.',1,0)\n",
    "\n",
    "# We don't need these columns\n",
    "to_drop = ['State','Phone','Churn?']\n",
    "churn_feat_space = churn_df.drop(to_drop,axis=1) #X\n",
    "\n",
    "# 'yes'/'no' has to be converted to boolean values\n",
    "# NumPy converts these from boolean to 1. and 0. later\n",
    "yes_no_cols = [\"Int'l Plan\",\"VMail Plan\"]\n",
    "churn_feat_space[yes_no_cols] = churn_feat_space[yes_no_cols] == 'yes'\n",
    "\n",
    "# Pull out features for future use\n",
    "features = churn_feat_space.columns\n",
    "\n",
    "X = churn_feat_space.values.astype(float)\n",
    "\n",
    "print (\"Feature space holds %d observations and %d features\" % X.shape)\n",
    "print (\"Unique target labels:\", np.unique(y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn import neighbors\n",
    "from sklearn import metrics\n",
    "acc = np.zeros((5,))\n",
    "i=0\n",
    "kf=model_selection.KFold(n_splits=5)\n",
    "kf.get_n_splits()\n",
    "#We will build the predicted y from the partial predictions on the test of each of the folds\n",
    "yhat = y.copy()\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    dt = neighbors.KNeighborsClassifier(n_neighbors=1)\n",
    "    dt.fit(X_train,y_train)\n",
    "    yhat[test_index] = dt.predict(X_test)\n",
    "    acc[i] = metrics.accuracy_score(yhat[test_index], y_test)\n",
    "    i=i+1\n",
    "print ('Mean accuracy: '+ str(np.mean(acc)))\n",
    "X_train[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_confusion(y,yhat,labels):\n",
    "    cm = metrics.confusion_matrix(y, yhat)\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.matshow(cm)\n",
    "    plt.title('Confusion matrix',size=20)\n",
    "    ax.set_xticklabels([''] + labels, size=20)\n",
    "    ax.set_yticklabels([''] + labels, size=20)\n",
    "    plt.ylabel('Predicted',size=20)\n",
    "    plt.xlabel('True',size=20)\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            ax.text(i, j, cm[i,j], va='center', ha='center',color='white',size=20)\n",
    "    fig.set_size_inches(7,7)\n",
    "    plt.show()\n",
    "\n",
    "draw_confusion(y,yhat,['no churn', 'churn'])\n",
    "print (metrics.classification_report(y,yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering: Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is quite a bad result. Remember that by always selecting class 'no churn' we should get around $85\\%$ of accuracy. As it was noticed before the definition of distance is critical. In NN we are using Euclidean distance. Distances assume that all variables operate at the same scale, i.e. all are commensurable. A change in one unit in one of the variables is equivalent to/as important as a change of 1 unit in the other. In this data set, this does not happen. For example, area codes values are around 400 while whether the customer enjoys an international plan take values 0 and 1. Thus, we may account for these changes by scaling the features. The most standard way of doing this is feature normalization or standarization. In this preprocessing technique each feature is scaled to have zero mean and unit standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standarize\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## In machine learning and data analysis, \"snooping\" refers to a scenario where information from outside the training dataset is inadvertently used to make decisions about the model.\n",
    "\n",
    "from sklearn import metrics\n",
    "acc_snooping = np.zeros((5,))\n",
    "i=0\n",
    "kf=model_selection.KFold(n_splits=5, shuffle=False)\n",
    "kf.get_n_splits()\n",
    "#We will build the predicted y from the partial predictions on the test of each of the folds\n",
    "yhat = y.copy()\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    dt = neighbors.KNeighborsClassifier(1)\n",
    "    dt.fit(X_train,y_train)\n",
    "    yhat[test_index] = dt.predict(X_test)\n",
    "    acc_snooping[i] = metrics.accuracy_score(yhat[test_index], y_test)\n",
    "    i=i+1\n",
    "print ('Mean accuracy: '+ str(np.mean(acc_snooping)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION:** In the former process we have accidentally snooped into the data and the result is contaminated. Where?</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Side Note: Data Snooping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NO SNOOPING\n",
    "acc = np.zeros((5,))\n",
    "i=0\n",
    "#We will build the predicted y from the partial predictions on the test of each of the folds\n",
    "yhat = y.copy()\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    dt = neighbors.KNeighborsClassifier(1)\n",
    "    dt.fit(X_train,y_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    yhat[test_index] = dt.predict(X_test)\n",
    "    acc[i] = metrics.accuracy_score(yhat[test_index], y_test)\n",
    "    i=i+1\n",
    "print ('Mean accuracy: '+ str(np.mean(acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acct=np.c_[acc_snooping,acc]\n",
    "plt.boxplot(acct);\n",
    "for i in range(2):\n",
    "    xderiv = (i+1)*np.ones(acct[:,i].shape)+(np.random.rand(5,)-0.5)*0.1\n",
    "    plt.plot(xderiv,acct[:,i],'ro',alpha=0.3)\n",
    "ax = plt.gca()\n",
    "ax.set_xticklabels(['snooping', 'no snooping'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_confusion(y,yhat,labels):\n",
    "    cm = metrics.confusion_matrix(y, yhat)\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.matshow(cm)\n",
    "    plt.title('Confusion matrix',size=20)\n",
    "    ax.set_xticklabels([''] + labels, size=20)\n",
    "    ax.set_yticklabels([''] + labels, size=20)\n",
    "    plt.ylabel('Predicted',size=20)\n",
    "    plt.xlabel('True',size=20)\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            ax.text(i, j, cm[i,j], va='center', ha='center',color='white',size=20)\n",
    "    fig.set_size_inches(7,7)\n",
    "    plt.show()\n",
    "\n",
    "draw_confusion(y,yhat,['no churn', 'churn'])\n",
    "print (metrics.classification_report(y,yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metrics from Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result is much better. As we have seen accuracy can be a little informative in some problems. For this reason we may use other performance measures. Classic performance measures can be derived from the confusion matrix. Consider the following confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_confusion(y,yhat,labels):\n",
    "    cm = metrics.confusion_matrix(y, yhat)\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.matshow(cm)\n",
    "    plt.title('Confusion matrix',size=20)\n",
    "    ax.set_xticklabels([''] + labels, size=20)\n",
    "    ax.set_yticklabels([''] + labels, size=20)\n",
    "    plt.ylabel('Predicted',size=20)\n",
    "    plt.xlabel('True',size=20)\n",
    "    ax.text(0, 0, 'TP', va='center', ha='center',color='white',size=20)\n",
    "    ax.text(0, 1, 'FN', va='center', ha='center',color='white',size=20)\n",
    "    ax.text(1, 0, 'FP', va='center', ha='center',color='white',size=20)\n",
    "    ax.text(1, 1, 'TN', va='center', ha='center',color='white',size=20)            \n",
    "    fig.set_size_inches(7,7)\n",
    "    plt.show()\n",
    "\n",
    "draw_confusion(y,yhat,['positive', 'negative'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix is divided in four quarters and contains\n",
    "\n",
    "+ True Positives (TP): Positive samples predicted as such.\n",
    "+ True Negatives (TN): Negative samples predicted as such.\n",
    "+ False Positives (FP): Negative samples predicted as positive.\n",
    "+ False Negatives (FN): Positive samples predicted as negative.\n",
    "\n",
    "The combination of these elements allows to define several performance metrics:\n",
    "\n",
    "+ Accuracy: \n",
    "\n",
    "$$\\text{accuracy}=\\frac{\\text{TP}+\\text{TN}}{\\text{TP}+\\text{TN}+\\text{FP}+\\text{FN}}$$\n",
    "\n",
    "Column-wise we find these two partial performance metrics:\n",
    "\n",
    "+ Sensitivity or Recall: \n",
    "\n",
    "$$\\text{sensitivity}=\\frac{\\text{TP}}{\\text{Real Positives}}=\\frac{\\text{TP}}{\\text{TP}+\\text{FN}}$$\n",
    "\n",
    "+ Specificity:\n",
    "\n",
    "$$\\text{specificity}=\\frac{\\text{TN}}{\\text{Real Negatives}}=\\frac{\\text{TN}}{\\text{TN}+\\text{FP}}$$\n",
    "\n",
    "Row-wise we find these two partial performance metrics:\n",
    "\n",
    "+ Precision or Positive Predictive Value:\n",
    "\n",
    "$$\\text{precision}=\\frac{\\text{TP}}{\\text{Predicted Positives}}=\\frac{\\text{TP}}{\\text{TP}+\\text{FP}}$$\n",
    "\n",
    "+ Negative predictive value:\n",
    "\n",
    "$$\\text{NPV}=\\frac{\\text{TN}}{\\text{Predicted Negative}}=\\frac{\\text{TN}}{\\text{TN}+\\text{FN}}$$\n",
    "\n",
    "The concept of positive and negative samples is purely arbitrary, thus we really have to remember the concepts of precision/positive predictive value and sensitivity/recall. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us check the concepts with churn as the positive class\n",
    "TP = np.sum(np.logical_and(yhat==1,y==1))\n",
    "TN = np.sum(np.logical_and(yhat==0,y==0))\n",
    "FP = np.sum(np.logical_and(yhat==1,y==0))\n",
    "FN = np.sum(np.logical_and(yhat==0,y==1))\n",
    "\n",
    "print ('TP: ' + str(TP))\n",
    "print ('TN: ' + str(TN))\n",
    "print ('FP: ' + str(FP))\n",
    "print ('FN: ' + str(FN))\n",
    "print ('sensitivity/recall: '+ str(TP/(TP+FN)))\n",
    "print ('precision: '+ str(TP/(TP+FP)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <ins>When to Use Recall and Precision</ins>\n",
    "\n",
    "In the context of classification tasks, understanding when to prioritize recall or precision can greatly impact the effectiveness of your model, especially in different real-world scenarios.\n",
    "\n",
    "#### Recall (Sensitivity)\n",
    "\n",
    "- **When to Use**: Recall should be prioritized when the cost of false negatives is high. In other words, when it's crucial to capture as many positive instances as possible.\n",
    "- **Example Scenario**: Medical diagnostics where failing to detect a disease (false negative) could be life-threatening. In such cases, it's better to have some false alarms (false positives) than to miss a positive case.\n",
    "  \n",
    "$$\\text{recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}$$\n",
    "\n",
    "#### Precision (Positive Predictive Value)\n",
    "\n",
    "- **When to Use**: Precision should be prioritized when the cost of false positives is high. This is important when the goal is to be as accurate as possible with the positive predictions.\n",
    "- **Example Scenario**: Email spam detection where it's more disruptive to wrongly classify important emails as spam (false positives) than to miss some spam emails (false negatives).\n",
    "\n",
    "$$\\text{precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}}$$\n",
    "\n",
    "#### Specificity\n",
    "\n",
    "- **When to Use**: Specificity should be prioritized when it's important to capture true negatives. This metric is crucial when the presence of a condition is to be ruled out with certainty.\n",
    "- **Example Scenario**: Screening a rare disease in a large population, where it's important to identify those who definitely don't have the disease.\n",
    "\n",
    "$$\\text{specificity} = \\frac{\\text{TN}}{\\text{TN} + \\text{FP}}$$\n",
    "\n",
    "#### Choosing the Right Metric\n",
    "\n",
    "Deciding whether to prioritize recall or precision (or specificity) often depends on the relative costs of false positives versus false negatives:\n",
    "\n",
    "- **High Cost of False Negatives**: Prioritize recall.\n",
    "- **High Cost of False Positives**: Prioritize precision.\n",
    "- **Balanced Approach Needed**: Consider using the F1-score, which is the harmonic mean of precision and recall, or the AUC-ROC curve, which considers both true positive rate (recall) and false positive rate (1 - specificity).\n",
    "\n",
    "Remember, the choice of metric should align with the business objectives or the specific requirements of the task at hand.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyzing the confusion matrix in our problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to predict custormer churn, thus we may ask how often the classifier correctly predicts it. We will consider \"churn\" as the positive class. The question we are wondering about is the ratio between the $TP$ and all the $\\text{Real Positives}$. This is the *sensitivity* or *recall*. We are able to correctly predict $187/(187+296) = 0.39$ of the customers that cease the service. Observe that this value is consistent with the classification report when checking recall for class $1$.\n",
    "\n",
    "However, we have to trade-off this value with *precision*. Precision answers the question, from all the customers we predict will churn, which is the ratio of those that actually churn? This effectively tells us the price we are paying in terms of how many non-churn customers are being predicted as quitters. If we check this value, we can see it is $187/(187+59) = 76\\%$. This means that about 1 out 4 customers predicted as churn are not quitting the service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nearest Neighbors**\n",
    "<p>\n",
    "<ul>\n",
    "<li> One of the simplest classifiers.\n",
    "<li> Smoothness of the model is governed by the number of the neighbors.\n",
    "<li> Hyper-parameter $k$ or $p$ in the $\\ell_p$ norm are set by cross-validation.\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supervised Learning Class\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "import pandas as pd\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "print(\"cancer.keys(): \\n{}\".format(cancer.keys()))\n",
    "print(cancer['target_names'])\n",
    "\n",
    "data = pd.DataFrame(cancer['data'], columns = cancer['feature_names'])\n",
    "target = pd.DataFrame(cancer['target'], columns = ['target'])\n",
    "data.head()\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "## train vs cross validation\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer['data'], cancer['target'], random_state=0)\n",
    "\n",
    "model = LogisticRegression()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(model.score(X_train, y_train))\n",
    "print(cross_validate(model,X_train, y_train, cv = 5)['test_score'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to Decision Trees in Classification\n",
    "\n",
    "Decision trees are a popular machine learning technique used for both classification and regression tasks. They are especially known for their interpretability and ease of use. A decision tree operates by breaking down a dataset into smaller and smaller subsets based on different criteria, and decisions are made based on those subdivisions.\n",
    "\n",
    "### Basic Idea\n",
    "\n",
    "The core **idea** behind decision trees is the \"divide and conquer\" approach. This involves:\n",
    "\n",
    "1. **Partitioning the space**: The dataset's feature space is divided into distinct regions. For classification tasks, these regions are chosen to best separate the different classes.\n",
    "2. **Fitting a model in each patch**: After partitioning, a simple model (or decision) is applied in each region.\n",
    "\n",
    "In **classification trees**, each partition (or \"leaf\") represents a class, and all data points that fall into a region are assigned the corresponding class label.\n",
    "\n",
    "### Key Concepts in Decision Trees\n",
    "\n",
    "#### Gini Index\n",
    "\n",
    "- The Gini index is a metric used to measure how often a randomly chosen element from the set would be incorrectly labeled if it was randomly labeled according to the distribution of labels in the subset. Gini index can be represented as:\n",
    "\n",
    "  $$\\text{Gini}(D) = 1 - \\sum_{i=1}^m p_i^2$$\n",
    "\n",
    "  where $D$ is the dataset and $p_i$ is the proportion of the elements labeled with the current class.\n",
    "\n",
    "- **Usage**: During the construction of a tree, the Gini index is used to evaluate the partitions, and the decision tree algorithm will favor splits that result in regions with the lowest Gini index — that is, the most homogeneous groups.\n",
    "\n",
    "#### Entropy and Information Gain\n",
    "\n",
    "- **Entropy** is another measure used to determine how a space should be split. It quantifies the amount of uncertainty or randomness in the data. Entropy can be calculated as:\n",
    "\n",
    "  $$\\text{Entropy}(S) = - \\sum_{i=1}^m p_i \\log_2 p_i$$\n",
    "\n",
    "- **Information Gain** is the reduction in entropy after a dataset is split on an attribute. It is used to decide which feature to split on at each step in building the tree.\n",
    "\n",
    "#### Tree Pruning\n",
    "\n",
    "- To avoid overfitting, decision trees need to control their growth. **Pruning** is a technique that removes parts of the tree to make it simpler and prevent it from modeling noise in the training data.\n",
    "\n",
    "#### Advantages and Limitations\n",
    "\n",
    "- **Advantages**: Decision trees are easy to understand and interpret, can handle both numerical and categorical data, and require relatively little data preparation.\n",
    "- **Limitations**: They can create overly complex trees that do not generalize well from the training data (overfitting), and small variations in the data can result in different"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Decision tree modeling\n",
    "\n",
    "Elements:\n",
    "\n",
    "- Splits using axis-orthogonal hyperplanes. This is the key that allows interpretability of the results.\n",
    "\n",
    "- At each internal node we test a value of a feature. A feature and a threshold are stored for each internal node.  \n",
    "\n",
    "- Leaves makes the class prediction. If leaves are pure, we have to store the class label. If leaves are impure, then the fraction of samples for each class is stored and its frequency is returned when queried.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building our intuition on decision trees\n",
    "\n",
    "Let us build up our intuition with a simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#Let's see what the boundary looks like in a toy problem.\n",
    "%reset -f\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "MAXN=10\n",
    "np.random.seed(2)\n",
    "X = np.concatenate([1.25*np.random.randn(MAXN,2),5+1.5*np.random.randn(MAXN,2)]) \n",
    "X = np.concatenate([X,[8,5]+1.5*np.random.randn(MAXN,2)])\n",
    "y = np.concatenate([np.ones((MAXN,1)),-np.ones((MAXN,1))])\n",
    "y = np.concatenate([y,np.ones((MAXN,1))])\n",
    "idxplus = y==1\n",
    "idxminus = y==-1\n",
    "plt.scatter(X[idxplus.ravel(),0],X[idxplus.ravel(),1],color='r')\n",
    "plt.scatter(X[idxminus.ravel(),0],X[idxminus.ravel(),1],color='b')\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn import metrics\n",
    "\n",
    "delta = 0.05\n",
    "xx = np.arange(-5.0, 15.0, delta)\n",
    "yy = np.arange(-5.0, 15.0, delta)\n",
    "XX, YY = np.meshgrid(xx, yy)\n",
    "Xf = XX.flatten()\n",
    "Yf = YY.flatten()\n",
    "sz=XX.shape\n",
    "data = np.c_[Xf[:,np.newaxis],Yf[:,np.newaxis]];\n",
    "clf = tree.DecisionTreeClassifier(random_state=0)\n",
    "clf.fit(X,y.ravel())\n",
    "Z=clf.predict(data)\n",
    "Z.shape=sz\n",
    "\n",
    "plt.imshow(Z, interpolation='bilinear', origin='lower', extent=(-5,15,-5,15),alpha=0.3, vmin=-1, vmax=1)\n",
    "plt.contour(XX,YY,Z,[0])\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(9,9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!dot -V\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install scikit-learn graphviz pydot\n",
    "#Export Tree\n",
    "import os\n",
    "dotfile = tree.export_graphviz(clf, out_file = \"toy_tree.dot\")\n",
    "\n",
    "os.system(\"dot -Tpng toy_tree.dot -o toy_tree.png\")\n",
    "\n",
    "from IPython.core.display import Image\n",
    "Image(\"toy_tree.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check the meaning of the tree. The first node splits the training set using feature $1$ by applying the threshold $\\leq 3.04$. As a result we are able to correctly classify eleven of the thirty data points. Let us see the boundary in that case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = tree.DecisionTreeClassifier(random_state=0,max_depth=1)\n",
    "clf.fit(X,y.ravel())\n",
    "Z=clf.predict(data)\n",
    "Z.shape=sz\n",
    "\n",
    "plt.scatter(X[idxplus.ravel(),0],X[idxplus.ravel(),1],color='r')\n",
    "plt.scatter(X[idxminus.ravel(),0],X[idxminus.ravel(),1],color='b')\n",
    "\n",
    "\n",
    "plt.imshow(Z, interpolation='bilinear', origin='lower', extent=(-5,15,-5,15),alpha=0.3, vmin=-1, vmax=1)\n",
    "plt.contour(XX,YY,Z,[0])\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(9,9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second node splits the training set using feature $0$ by applying the threshold $\\leq 6.25$. Note that this only is used in the part of the space where feature $1$ is greater  than $3.04$. Observe that the remaining blue space is characterized by the following logical function: $(x_1>3.04) \\wedge (x_0\\leq 6.25)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = tree.DecisionTreeClassifier(random_state=0, max_depth=2)\n",
    "clf.fit(X,y.ravel())\n",
    "Z=clf.predict(data)\n",
    "Z.shape=sz\n",
    "\n",
    "plt.scatter(X[idxplus.ravel(),0],X[idxplus.ravel(),1],color='r')\n",
    "plt.scatter(X[idxminus.ravel(),0],X[idxminus.ravel(),1],color='b')\n",
    "\n",
    "plt.imshow(Z, interpolation='bilinear', origin='lower', extent=(-5,15,-5,15),alpha=0.3, vmin=-1, vmax=1)\n",
    "plt.contour(XX,YY,Z,[0])\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(9,9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is great about decision trees?\n",
    "\n",
    "+ Trees are easy for humans to interpret. It can be seen as a set of rules. Each path from root to one leaf of the tree is an AND combination of the thresholded features.\n",
    "+ Given a finite data set, decision trees can express any function of the input attributes. In ${\\bf R}^d$ we can isolate every point in the data set by constructing a box around each of them.\n",
    "+ There can be more than one tree that fits the same data. From all of them we would like a tree with minimum number of nodes. But the problem is NP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning the tree\n",
    "\n",
    "Because the problem is NP we can resort to a greedy construction algorithm. Greedy algorithms choose the current best binary partition without taking into account its impact on the quality of subsequent splits.\n",
    "\n",
    "The algorithm idea is as follows:\n",
    "\n",
    "+ Initialize the algorithm with a node associated to the full data set. \n",
    "\n",
    "**while** the list is not empty\n",
    "1. Retrieve the first node from the list.\n",
    "2. Find the data associated to that node.\n",
    "3. Find a splitting point.\n",
    "4. If the node is splittable, create the nodes linked to the parent node and put them in the exploration list.\n",
    "\n",
    "#### The splitting criterion\n",
    "\n",
    "There are many different splitting criteria. The most common ones are:\n",
    "\n",
    "+ Misclassification error\n",
    "+ Gini index\n",
    "+ Cross-entropy/Information gain/Mutual information\n",
    "\n",
    "Withouth going into details: \n",
    "\n",
    " - Misclassification error splits greedily select the split that corrects more data at each point. \n",
    " - Gini index and cross-entropy probabilistically model the notion of impurity of a node. The split is chosen so that the average purity of the new nodes is maximized. Observe that as we descend in the tree the purity increases and eventually converge to pure leaves.\n",
    " - Entropy measures the average surprise/information a probabilistic result yields. In a binary variable, the maximum surprise occurs when both outcomes are equally probable, one has the maximum uncertainty on the result. Otherwise, the surprise decreases. This behavior is also display in Gini's index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "entropy = lambda p: -np.sum(p * np.log2(p)) if not 0 in p else 0\n",
    "gini = lambda p: 1. - (np.array(p)**2).sum()\n",
    "pvals = np.linspace(0, 1)        \n",
    "plt.plot(pvals, [entropy([p,1-p])/2. for p in pvals], label='Entropy')\n",
    "plt.plot(pvals, [gini([p,1-p]) for p in pvals], label='Gini')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trees and overfitting\n",
    "\n",
    "Because trees are very expressive models they can model any training set perfectly and easily overfit.\n",
    "\n",
    "There are two ways of avoiding overfitting in trees:\n",
    "\n",
    "+ Stop growing the tree when the split is not statistically significant.\n",
    "+ Grow a full tree and post-prune.\n",
    "\n",
    "One of the simplest ways of post pruning is \"reduced error prunning\". It goes like this,\n",
    "\n",
    "1. Split data into training and validation\n",
    "2. Create a candidate tree on the training set\n",
    "3. Do until further pruning is harmful\n",
    "    1. Evaluate impact on the validation set of removing each posible node (with descendants)\n",
    "    2. Greedily remove the node that improves the performance the most.\n",
    "    \n",
    "Pruning is not implemented in sklearn at this moment. However let us check what happens in our customer churn prediction problem when we use a decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "#Recover Churn data\n",
    "import pickle\n",
    "fname = open('churn_data.pkl','rb')\n",
    "data = pickle.load(fname)\n",
    "X = data[0]\n",
    "y = data[1]\n",
    "features = data[2]\n",
    "print ('Loading ok.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NO SNOOPING\n",
    "import numpy as np\n",
    "from sklearn import model_selection\n",
    "from sklearn import tree\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "kf=model_selection.KFold(n_splits=5, shuffle=False)\n",
    "kf.get_n_splits()\n",
    "acc = np.zeros((5,))\n",
    "i=0\n",
    "#We will build the predicted y from the partial predictions on the test of each of the folds\n",
    "yhat = y.copy()\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    dt = tree.DecisionTreeClassifier(criterion='entropy')\n",
    "    dt.fit(X_train,y_train)\n",
    "\n",
    "    yhat[test_index] = dt.predict(X_test)\n",
    "    acc[i] = metrics.accuracy_score(yhat[test_index], y_test)\n",
    "    i=i+1\n",
    "print ('Mean accuracy: '+ str(np.mean(acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def draw_confusion(y,yhat,labels):\n",
    "    cm = metrics.confusion_matrix(y, yhat)\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.matshow(cm)\n",
    "    plt.title('Confusion matrix',size=20)\n",
    "    ax.set_xticklabels([''] + labels, size=20)\n",
    "    ax.set_yticklabels([''] + labels, size=20)\n",
    "    plt.ylabel('Predicted',size=20)\n",
    "    plt.xlabel('True',size=20)\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            ax.text(i, j, cm[i,j], va='center', ha='center',color='white',size=20)\n",
    "    fig.set_size_inches(7,7)\n",
    "    plt.show()\n",
    "\n",
    "draw_confusion(y,yhat,['no churn', 'churn'])\n",
    "print (metrics.classification_report(y,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us check the concepts with churn as the positive class\n",
    "TP = np.sum(np.logical_and(yhat==1,y==1))\n",
    "TN = np.sum(np.logical_and(yhat==0,y==0))\n",
    "FP = np.sum(np.logical_and(yhat==1,y==0))\n",
    "FN = np.sum(np.logical_and(yhat==0,y==1))\n",
    "\n",
    "print ('TP: ' + str(TP))\n",
    "print ('TN: ' + str(TN))\n",
    "print ('FP: ' + str(FP))\n",
    "print ('FN: ' + str(FN))\n",
    "print ('sensitivity/recall: '+ str(TP/(TP+FN)))\n",
    "print ('precision: '+ str(TP/(TP+FP)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that by using a decision tree, the recall increased by $30\\%$ while having the precision at a simliar level than nearest neighbors. Let us check the first levels of the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#Let us check the the first three levels of the tree. GraphViz and PyDot are needed.\n",
    "dt = tree.DecisionTreeClassifier(criterion='entropy', max_depth=3)\n",
    "#scaler = StandardScaler()\n",
    "#Xs = scaler.fit_transform(X)\n",
    "dt.fit(X,y)\n",
    "\n",
    "import pandas as pd\n",
    "display(pd.DataFrame(X, columns = features))\n",
    "\n",
    "#Export Tree\n",
    "\n",
    "dotfile = tree.export_graphviz(dt, out_file = \"churn.dot\", feature_names = features)\n",
    "\n",
    "os.system(\"dot -Tpng churn.dot -o churn.png\")\n",
    "from IPython.core.display import Image\n",
    "Image(\"churn.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe the first feature split and the values of the entropy according to the split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy = lambda p: -np.sum(p * np.log2(p)) if not 0 in p else 0\n",
    "\n",
    "#Let us check the entropy on the root node\n",
    "#There are 2850 samples of customers that stay in the company. The frequency is\n",
    "proot = 2850/3333\n",
    "#And the entropy value is\n",
    "print( 'Root node entropy: '+ str(entropy([proot,1-proot])))\n",
    "\n",
    "#After the split we have the following frequencies for the left and right children\n",
    "pleft = 2766/3122 #Frequency of label 0 on the left node\n",
    "pright = 84/211 #Frequency of label 0 on the right node\n",
    "\n",
    "print( 'Left node entropy: '+ str(entropy([pleft,1-pleft])))\n",
    "print ('Right node entropy: '+ str(entropy([pright,1-pright])))\n",
    "print(\"Weighted mean of Entropy: \", entropy([pleft,1-pleft])*3122/3333 + entropy([pright,1-pright])*211/3333)\n",
    "\n",
    "#Information gain computes the difference between the entropy of the parent and the weighed entropies of the children\n",
    "# I = H_root - \\sum freq_i * H_i\n",
    "\n",
    "I = entropy([proot,1-proot]) - 3122/3333*entropy([pleft,1-pleft])+211/3333*entropy([pright,1-pright])\n",
    "\n",
    "print ('Information gain: '+ str(I))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The split reduces the average entropy of the children, thus the splits are more pure.\n",
    "\n",
    "Observations:\n",
    "\n",
    "+ Observe that because we have restricted (for visualization purposes) the maximum depth of the tree, the leaves are not pure. \n",
    "+ Analyzing the leaves we can see that most of the clients that are hooked to the plan share the following conditions:\n",
    "\n",
    "$$(\\text{Day Charge} \\geq 44.96) \\wedge (\\text{VMail Plan} = \\text{YES}) \\wedge (\\text{International Plan} = \\text{YES})$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian models (Naive Bayes) and some applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayesian models (Naive Bayes) and some applications\n",
    "\n",
    "Imagine we have a big basket of fruits, and we want to sort them into types like apples, bananas, and oranges. This task is similar to what the Naive Bayes classifier does in the world of machine learning, but instead of sorting fruits, it sorts information.\n",
    "\n",
    "#### What is Naive Bayes?\n",
    "\n",
    "Naive Bayes is like a smart sorting machine that helps us organize things based on what it knows about them. For our fruit example, think of Naive Bayes as a friend who helps you sort fruits by telling you which fruit is likely to be an apple, a banana, or an orange, based on their characteristics like color, shape, and taste.\n",
    "\n",
    "- **Bayes' Theorem**: This is the rule our sorting machine uses. It's a mathematical formula that helps predict the type of fruit based on its features. For instance, if you tell it a fruit is long and yellow, Bayes' Theorem helps it guess that it's probably a banana.\n",
    "\n",
    "- **Naive Assumption**: The \"naive\" part comes from the machine treating each feature (like color or shape) as if it's completely independent of the others. Even though this isn't how things work in real life (for example, oranges are both orange and round), this assumption makes our machine very fast and surprisingly good at guessing!\n",
    "\n",
    "#### How Does Naive Bayes Sort Fruits?\n",
    "\n",
    "Let's break it down into simple steps:\n",
    "\n",
    "1. **Learning**: First, you show the machine lots of different fruits and tell it what they are. This is like teaching it the game by showing it examples.\n",
    "\n",
    "2. **Guessing**: After learning, when you give the machine a new fruit it hasn't seen before, it uses what it learned to guess the fruit's type. It looks at the fruit's features (color, shape, taste) and calculates which type it's most likely to be.\n",
    "\n",
    "##### Seeing Naive Bayes in Action\n",
    "\n",
    "Imagine you draw circles around apples, bananas, and oranges based on their features. Naive Bayes does something similar in its mind. When a new fruit comes along, it sees where the fruit fits best based on the circles drawn from what it learned.\n",
    "\n",
    "#### Why is Naive Bayes So Special?\n",
    "\n",
    "Even though it makes simple assumptions, Naive Bayes can quickly sort through lots of information (or fruits) and make accurate guesses. It's like having a super-fast fruit sorter that gets better the more it learns.\n",
    "\n",
    "Next, we'll see how we can use this amazing sorter not just for fruits, but for sorting all kinds of things, making our lives easier and more organized. We'll take it step by step, ensuring it's fun and easy to understand.\n",
    "\n",
    "[Naive Bayes explained](https://www.youtube.com/watch?v=O2L2Uv9pdDA&ab_channel=StatQuestwithJoshStarmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the environment to ensure a clean slate\n",
    "%reset -f\n",
    "\n",
    "# Enable plotting in the Jupyter Notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create synthetic data for two groups\n",
    "# The data for each group is generated around a different center point to simulate distinction\n",
    "X = np.concatenate([1.25*np.random.randn(40,2), 5+1.5*np.random.randn(40,2)]) \n",
    "y = np.concatenate([np.ones((40,1)), -np.ones((40,1))])\n",
    "\n",
    "# Initialize the Gaussian Naive Bayes classifier\n",
    "nb = GaussianNB()\n",
    "# Train the classifier with the synthetic data\n",
    "nb.fit(X, y.ravel())\n",
    "\n",
    "# Create a grid of points to visualize the decision boundary\n",
    "delta = 0.025\n",
    "xx = np.arange(-5.0, 10.0, delta)\n",
    "yy = np.arange(-5.0, 10.0, delta)\n",
    "XX, YY = np.meshgrid(xx, yy)\n",
    "\n",
    "# Predict probabilities on the grid to visualize the decision boundary\n",
    "Z = nb.predict_proba(np.c_[XX.ravel(), YY.ravel()])\n",
    "Z = Z[:, 1].reshape(XX.shape)\n",
    "\n",
    "# Plotting\n",
    "plt.figure()\n",
    "# Separate the data into two groups for plotting\n",
    "idxplus = y == 1\n",
    "idxminus = y == -1\n",
    "idxplus = idxplus.flatten()\n",
    "idxminus = idxminus.flatten()\n",
    "# Plot the first group in red\n",
    "plt.scatter(X[idxplus, 0], X[idxplus, 1], color='r', alpha=0.4)\n",
    "# Plot the second group in blue\n",
    "plt.scatter(X[idxminus, 0], X[idxminus, 1], color='b', alpha=0.4)\n",
    "# Overlay the probability density to visualize the decision boundary\n",
    "plt.imshow(Z, interpolation='bilinear', origin='lower', extent=(-5, 10, -5, 10), alpha=0.3, vmin=0, vmax=1)\n",
    "# Draw the contour line where the probability is 0.5 (decision boundary)\n",
    "plt.contour(XX, YY, Z, [0.5])\n",
    "# Adjust the figure size for better visibility\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(9, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">If the boundary is not linear why is it considered a linear model? It is not a linear model, though it is an affine model with respect to the weights. In the particular case of text classification we will use certain probability density functions that will make the model linear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Gaussian Naive Bayes classifier, a cornerstone of machine learning, particularly shines in its simplicity and effectiveness in classification tasks. Through the Python example provided, we've visualized not just the classification process but also how Naive Bayes makes decisions based on probabilities.\n",
    "\n",
    "#### Key Takeaways\n",
    "\n",
    "- **Simplicity in Action**: The Naive Bayes classifier, despite its underlying assumption of feature independence, demonstrates robust performance in classifying data into distinct groups. This example, using synthetic data, showcases how even with basic assumptions, significant insights can be gleaned.\n",
    "\n",
    "- **Visualization of Decision Boundaries**: By plotting the data points and the decision boundary, we gain a visual understanding of how Naive Bayes classifies data. The contour line at which the probability is 0.5 acts as the threshold—data points falling on one side of this line belong to one category, and those on the other side belong to another.\n",
    "\n",
    "- **Probabilistic Approach**: The background gradient in the visualization represents the probability of belonging to one of the categories. This gradient illustrates the probabilistic nature of Naive Bayes, offering more than just classifications—it provides insights into the certainty of its predictions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10.1.1 Basic document representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine you're sorting through a pile of different stories, and you want to organize them into groups like \"adventure tales\" or \"fairy tales.\" This is what we call **text classification**, and we use a method called Naive Bayes to help us figure out which group each story belongs to.\n",
    "\n",
    "#### How Do We Understand Stories?\n",
    "\n",
    "To help Naive Bayes sort stories, we first need to translate the stories into a format it can understand. We do this using a **bag-of-words** approach. Think of this as taking each story and breaking it down into a list of words that we're interested in.\n",
    "\n",
    "- **Bag-of-Words Example**: Suppose we're interested in stories about \"pirates\" and \"wizards.\" We decide to focus on certain words like \"ship,\" \"treasure,\" \"wand,\" and \"spell.\" We then see how many times these words appear in each story.\n",
    "\n",
    "| Story | Ship | Treasure | Wand | Spell |\n",
    "|-------|------|----------|------|-------|\n",
    "| 1 (Pirates) | 3    | 2        | 0    | 0     |\n",
    "| 2 (Pirates) | 2    | 1        | 0    | 1     |\n",
    "| 3 (Wizards) | 0    | 0        | 2    | 3     |\n",
    "| 4 (Wizards) | 0    | 1        | 3    | 2     |\n",
    "\n",
    "- In this table, Story 1 has 3 \"ship\" words, 2 \"treasure\" words, and none of the \"wand\" or \"spell\" words, which helps Naive Bayes guess it's a pirate story.\n",
    "\n",
    "#### How Naive Bayes Makes Its Guess\n",
    "\n",
    "Naive Bayes looks at the word counts for each story. If a story has more \"pirate\" words like \"ship\" and \"treasure,\" it guesses it's about pirates. If it has more \"wizard\" words like \"wand\" and \"spell,\" it guesses it's about wizards.\n",
    "\n",
    "#### Key Points to Remember\n",
    "\n",
    "- **Counting vs. Presence**: Sometimes, just knowing whether a word appears in the story or not (yes/no) is enough, instead of counting how many times it appears.\n",
    "- **Word Order Doesn't Matter**: With bag-of-words, the order of words is ignored. \"The treasure is on the ship\" and \"The ship is on the treasure\" would be seen as the same. Although they mean different things, for our purpose of sorting stories, this simple approach can still be very effective.\n",
    "\n",
    "By transforming stories into bags of words, Naive Bayes assists us in categorizing them into themes like \"pirates\" or \"wizards.\" Despite its simplicity, this method is a powerful tool for helping computers understand and organize stories, much like we do.\n",
    "\n",
    "#### Understanding the Naive Bayes Classifier in Document Classification\n",
    "\n",
    "Naive Bayes is a straightforward yet powerful approach used in predicting the category of documents. It's like playing a guessing game where, based on the words used in a document, we try to predict its topic, such as whether it's about \"economics\" or \"technology.\"\n",
    "\n",
    "#### How Naive Bayes Works\n",
    "\n",
    "At its heart, Naive Bayes selects the category that is most likely to be correct based on the words present in the document. This is done using a mathematical formula known as Bayes' Theorem:\n",
    "\n",
    "- To find out which category is the most probable, we use this formula:\n",
    "\n",
    "  $$\\hat{y} = \\arg\\max_y p(y|x).$$\n",
    "\n",
    "  Here, $\\hat{y}$ is our best guess for the category of the document, $y$ represents a possible category, and $x$ is the description of the document (like the words it contains).\n",
    "\n",
    "- Bayes' Theorem helps us in this guessing game by relating different probabilities:\n",
    "\n",
    "  $$p(y|x) = \\frac{p(x|y)p(y)}{p(x)}.$$\n",
    "\n",
    "  In simple terms, this tells us how likely a category is given the document's description. We calculate this by looking at how common the words are in each category ($p(x|y)$), how common each category is ($p(y)$), and how common these words are in all documents ($p(x)$).\n",
    "\n",
    "#### Simplifying the Process\n",
    "\n",
    "When we're classifying documents, we're essentially comparing which category is more likely based on the words used. For instance, if a document contains words more common in \"economics\" than in \"technology,\" it will be classified under \"economics.\"\n",
    "\n",
    "- Interestingly, we don't need to worry about $p(x)$, the probability of seeing a particular set of words, because it doesn't change our decision. This simplifies our formula to:\n",
    "\n",
    "  $$P(y|x) \\propto P(y)P(x|y)$$\n",
    "\n",
    "  This means we're mostly interested in how likely a category is ($P(y)$) and how likely we are to see these words if the document is in that category ($P(x|y)$).\n",
    "\n",
    "#### Naive Bayes' Special Assumption\n",
    "\n",
    "What makes Naive Bayes \"naive\" is its assumption that each word in the document affects the category independently of other words. This assumption allows us to simply multiply the probabilities of individual words relating to a category to find the overall likelihood:\n",
    "\n",
    "$$p(x_1,x_2,...,x_N | y) = p(x_1|y)p(x_2|y)...p(x_N|y) = \\prod\\limits_{i=1}^N p(x_i|y)$$\n",
    "\n",
    "For example, the probability that a document is about \"technology\" given it contains certain words can be calculated by multiplying the probabilities of each of those words belonging to the \"technology\" category.\n",
    "\n",
    "#### Practical Application\n",
    "\n",
    "When applying Naive Bayes to document classification, we might not always know the prior probability of each category ($p(y)$). In such cases, we might treat all categories as equally likely or use what's known as a non-informative prior. This leads us to focus on the likelihood of words within categories, simplifying our task to finding the most likely category based on the words in the document.\n",
    "\n",
    "In summary, Naive Bayes helps us classify documents by comparing the probabilities of words within categories, making it a straightforward yet effective tool for understanding and organizing information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10.1.2 Estimating conditioned probabilities "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last remaining step is the estimation of the individual conditional probabilities. There are two classical variants the **Multinomial Naive Bayes** and the **Bernoulli Naive Bayes**. The difference between both lies in the goal of what they are modeling. **In Multinomial NB we compute the probability of generating the observed document.** In this sense, we multiply the conditional probability of each word in the document for all words present in the document. An alternative view is the *Bernoulli model*. **In the Bernoulli Naive Bayes we compute the probability of the binary bag-of-words descriptor.** Observe that in the Bernouilli Naive Bayes the final probability depends on the words that appear in the document but also on the words that do not appear while in the multinomial NB it only depends on the words that appear. On the contrary, multinomial naive bayes takes into account the multiplicity of the words in the document while Bernoulli does not. Let us consider in this example the *Bernoulli model* that is consistent with our representation where a zero indicates a word is not present in the document and a one represents it is present. In order to estimate this probability we can use a frequentist approximation to probability, i.e. we will estimate the probability as the frequency of appearance of each term in each category. This computation divides the number documents where the word appears over the total number of documents. \n",
    "\n",
    "In our previous example, $p(x_3=1 (\\text{the word 'price' appears})|y =\\text{'tech'}) = 1/2$ and $p(x_3=1 (\\text{the word 'price' appears})|y =\\text{'eco'}) = 2/2$. This is computed by dividing the number of documents where the word price appear in a given category over the number of documents of that category.\n",
    "\n",
    "#### The zero probability effect\n",
    "In the former example the probability $p(x_5=1|y=\\text{'eco'}) = 0$. This implies that if the word 'mobile' appears the document can not belong to the class $\\text{'economy'}$. It is unreasonable to completely penalize a whole class by the appearance or not appearance of a single word. It is customary to assign to those cases a very low probability value instead. One well known approach to correct this effect is the so called **Laplace correction**. It is computed as follows,\n",
    "\n",
    "$$p(x_i=1 | y=c_k ) = \\frac{\\text{# of documents of class } c_k \\text{ where word } x_i \\text{ appears} + 1}{\\text{# of documents of class } c_k + M}$$\n",
    "\n",
    "where $M$ is the amount of words in the description. \n",
    "\n",
    "#### Underflow effect\n",
    "\n",
    "As the number of words in the description increase there is a higher probability that many of those words will not be present in the document. The product of many very small values may lead to floating point underflow effects. For this reason it is usual to use the log probability instead. This transformation does not change the decision boundary. In our simplified case\n",
    "\n",
    "$$\\log p(x|y) = \\sum\\limits_{i=1}^N \\log p(x_i|y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10.1.3 Applying Naive Bayes to text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, our goal is to automatically categorize news according to their title into twenty-eight standard topics. In this problem we will deal with every New York Times front page story from 1996 to 2006, coded according to the Policy Agendas (http://www.policyagendas.org). This collection of data has been compiled by Amber E. Boydstun.\n",
    "\n",
    "Specifically, we are interested in classifying news from The New York Times in the following macro-topics according to its title:\n",
    "\n",
    "\n",
    "\n",
    "<table border=\"1\">\n",
    "<tr>\n",
    "<td>\n",
    "1 \n",
    "<td>\n",
    "Macroeconomics\n",
    "<tr>\n",
    "<td>\n",
    "2 \n",
    "<td>\n",
    "Civil Rights, Minority Issues, and Civil Liberties \n",
    "<tr>\n",
    "<td>\n",
    "3\n",
    "<td>\n",
    "Health\n",
    "<tr>\n",
    "<td>\n",
    "4 \n",
    "<td>Agriculture\n",
    "<tr>\n",
    "<td>\n",
    "5 \n",
    "<td>Labor, Employment, and Immigration\n",
    "<tr>\n",
    "<td>\n",
    "6 \n",
    "<td> Education\n",
    "<tr>\n",
    "<td>\n",
    "7\n",
    "<td>Environment\n",
    "<tr>\n",
    "<td>\n",
    "8\n",
    "<td>Energy\n",
    "<tr>\n",
    "<td>\n",
    "10 \n",
    "<td>Transportation\n",
    "<tr>\n",
    "<td>\n",
    "12 \n",
    "<td>Law, Crime, and Family Issues\n",
    "<tr>\n",
    "<td>\n",
    "13 \n",
    "<td>Social Welfare\n",
    "<tr>\n",
    "<td>\n",
    "14 \n",
    "<td>Community Development and Housing Issues\n",
    "<tr>\n",
    "<td>\n",
    "15 \n",
    "<td>Banking, Finance, and Domestic Commerce\n",
    "<tr>\n",
    "<td>\n",
    "16 \n",
    "<td>Defense\n",
    "<tr>\n",
    "<td>\n",
    "17 \n",
    "<td>Space, Science, Technology and Communications\n",
    "<tr>\n",
    "<td>\n",
    "18 \n",
    "<td>Foreign Trade\n",
    "<tr>\n",
    "<td>\n",
    "19 \n",
    "<td>International Affairs and Foreign Aid\n",
    "<tr>\n",
    "<td>\n",
    "20 \n",
    "<td>Government Operations\n",
    "<tr>\n",
    "<td>\n",
    "21 \n",
    "<td>Public Lands and Water Management\n",
    "<tr>\n",
    "<td>\n",
    "24 \n",
    "<td>State and Local Government Administration\n",
    "<tr>\n",
    "<td>\n",
    "26 \n",
    "<td>Weather and Natural Disasters\n",
    "<tr>\n",
    "<td>\n",
    "27 \n",
    "<td>Fires\n",
    "<tr>\n",
    "<td>\n",
    "28 \n",
    "<td>Arts and Entertainment\n",
    "<tr>\n",
    "<td>\n",
    "29 \n",
    "<td>Sports and Recreation\n",
    "<tr>\n",
    "<td>\n",
    "30 \n",
    "<td>Death Notices\n",
    "<tr>\n",
    "<td>\n",
    "31 \n",
    "<td>Churches and Religion\n",
    "<tr>\n",
    "<td>\n",
    "99 \n",
    "<td>Other, Miscellaneous, and Human Interest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "#load data\n",
    "import pandas as pd\n",
    "data=pd.read_csv('Boydstun_NYT_FrontPage_Dataset_1996-2006_0.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us split the data set in two set: \n",
    "    \n",
    "+ We will train the classifier with news up to 2004.\n",
    "+ We will test the classifier in news from 2005 and 2006."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Splitting the dataset based on date: Training data will be from before 1/1/2004, and testing data from 2004 to 2006\n",
    "split = pd.to_datetime(pd.Series(data['Date'])) < pd.Timestamp(2004, 1, 1)\n",
    "\n",
    "# Extracting the 'Title' column as our raw data\n",
    "raw_data = data['Title']\n",
    "\n",
    "# Splitting the titles into training and testing sets based on the date\n",
    "raw_train = raw_data[split]\n",
    "raw_test = raw_data[np.logical_not(split)]\n",
    "\n",
    "# Extracting the topic labels for our dataset\n",
    "y = data['Topic_2digit']\n",
    "\n",
    "# Splitting the labels into training and testing sets corresponding to our title splits\n",
    "y_train = y[split]\n",
    "y_test = y[np.logical_not(split)]\n",
    "\n",
    "# Printing out the sizes of our training and testing datasets to ensure the split was done correctly\n",
    "print('Check the split sizes, train, test and total amount of data:')\n",
    "print(raw_train.shape, raw_test.shape, raw_data.shape)\n",
    "\n",
    "# Displaying the unique labels in our dataset to understand the classification categories\n",
    "print('Display the labels:')\n",
    "print(np.unique(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary tool for text processing\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initializing the CountVectorizer with specific parameters\n",
    "vectorizer = CountVectorizer(\n",
    "    min_df=2,  # A word must appear in at least two documents to be considered\n",
    "    stop_words='english',  # Removing common English words (e.g., 'and', 'the', 'of') that don't contribute much to the meaning\n",
    "    strip_accents='unicode'  # Removing accents from characters for consistency\n",
    ")\n",
    "\n",
    "# Demonstrating the preprocessing and tokenization process with an example\n",
    "test_string = raw_train[0]  # Taking the first title from the training set as an example\n",
    "print(\"Example: \" + test_string + \"\\n\")\n",
    "# Showing the result of preprocessing (e.g., lowercasing, removing punctuation)\n",
    "print(\"Preprocessed: \" + vectorizer.build_preprocessor()(test_string) + \"\\n\")\n",
    "# Displaying the list of words (tokens) after splitting the preprocessed text\n",
    "print(\"Tokenized:\" + str(vectorizer.build_tokenizer()(test_string)) + \"\\n\")\n",
    "# Applying the full analyzer (preprocessing, tokenizing, and filtering stop words)\n",
    "print(\"Analyzed data string:\" + str(vectorizer.build_analyzer()(test_string)) + \"\\n\")\n",
    "\n",
    "# Processing the entire datasets to convert the raw text into a matrix of token counts\n",
    "X_train = vectorizer.fit_transform(raw_train)  # Learning the vocabulary and transforming the training set\n",
    "X_test = vectorizer.transform(raw_test)  # Transforming the test set based on the learned vocabulary\n",
    "\n",
    "# Printing the total number of tokens (unique words) found in the dataset\n",
    "print(\"Number of tokens: \" + str(len(vectorizer.get_feature_names_out())) + \"\\n\")\n",
    "# Displaying a slice of the tokens for inspection\n",
    "print(\"Extract of tokens:\")\n",
    "print(vectorizer.get_feature_names_out()[1000:1100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable plotting directly within the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# Import the Bernoulli Naive Bayes classifier from scikit-learn\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "# Initialize the classifier\n",
    "nb = BernoulliNB()\n",
    "\n",
    "# Train the classifier on the training data\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set\n",
    "y_hat = nb.predict(X_test)\n",
    "\n",
    "# Import necessary tools for evaluation\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define a function to plot the confusion matrix\n",
    "def plot_confusion_matrix(y_pred, y):\n",
    "    plt.imshow(metrics.confusion_matrix(y, y_pred), interpolation='nearest', cmap='gray')\n",
    "    plt.colorbar()\n",
    "    plt.ylabel('true value')\n",
    "    plt.xlabel('predicted value')\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(9, 9)\n",
    "\n",
    "# Print the classification accuracy: the proportion of correctly predicted instances\n",
    "print(\"classification accuracy:\", metrics.accuracy_score(y_hat, y_test))\n",
    "\n",
    "# Call the function to plot the confusion matrix for the test predictions\n",
    "plot_confusion_matrix(y_hat, y_test)\n",
    "\n",
    "# Print a detailed classification report showing precision, recall, f1-score, and support for each class\n",
    "print(\"Classification Report:\")\n",
    "print(metrics.classification_report(y_hat, np.array(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION:** Identify the three most simple classes.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save data for future use.\n",
    "import pickle\n",
    "ofname = open('NYT_data.pkl', 'wb')\n",
    "s = pickle.dump([X_train,y_train,X_test,y_test],ofname)\n",
    "ofname.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5\n",
    "voc = vectorizer.get_feature_names_out()\n",
    "for i, label in enumerate(np.unique(y)):\n",
    "    topN = np.argsort(nb.feature_log_prob_[i])[-N:]\n",
    "    print('Code: ' + str(label) + ' Terms: ' + str([voc[j] for j in topN]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check what would happen if we enrich the data set with the summary of the article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = data['Title']+data['Summary']\n",
    "raw_train = raw_data[split]\n",
    "raw_test = raw_data[np.logical_not(split)]\n",
    "y = data['Topic_2digit']\n",
    "y_train = y[split]\n",
    "y_test = y[np.logical_not(split)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us tokenize the data\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=2, \n",
    " stop_words='english', \n",
    " strip_accents='unicode')\n",
    "\n",
    "#example\n",
    "test_string = raw_train[0]\n",
    "print (\"Example: \" + test_string +\"\\n\")\n",
    "print (\"Preprocessed: \" + vectorizer.build_preprocessor()(test_string)+\"\\n\")\n",
    "print (\"Tokenized:\" + str(vectorizer.build_tokenizer()(test_string))+\"\\n\")\n",
    "print (\"Analyzed data string:\" + str(vectorizer.build_analyzer()(test_string))+\"\\n\")\n",
    "\n",
    "\n",
    "#Fit and convert data\n",
    "X_train = vectorizer.fit_transform(raw_train)\n",
    "X_test = vectorizer.transform(raw_test)\n",
    "\n",
    "print (\"\\n\")\n",
    "print (\"Number of tokens: \" + str(len(vectorizer.get_feature_names_out())) +\"\\n\")\n",
    "print (\"Extract of tokes:\")\n",
    "print( vectorizer.get_feature_names_out()[1000:1100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "nb = BernoulliNB()\n",
    "nb.fit(X_train,y_train)\n",
    "\n",
    "y_hat = nb.predict(X_test)\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_confusion_matrix(y_pred, y):\n",
    "    plt.imshow(metrics.confusion_matrix(y, y_pred), interpolation='nearest')\n",
    "    plt.colorbar()\n",
    "    plt.ylabel('true value')\n",
    "    plt.xlabel('predicted value')\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(9,9)    \n",
    "    \n",
    "print (\"classification accuracy:\", metrics.accuracy_score(y_hat, y_test))\n",
    "plot_confusion_matrix(y_hat, y_test)\n",
    "print (\"Classification Report:\")\n",
    "print (metrics.classification_report(y_hat,np.array(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save data for future use.\n",
    "import pickle\n",
    "ofname = open('NYT_context_data.pkl', 'wb')\n",
    "s = pickle.dump([X_train,y_train,X_test,y_test],ofname)\n",
    "ofname.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What are the top N most predictive features per class?\n",
    "N = 5\n",
    "voc = vectorizer.get_feature_names_out()\n",
    "for i, label in enumerate(np.unique(y)):\n",
    "    topN = np.argsort(nb.feature_log_prob_[i])[-N:]\n",
    "    print ('Code: '+ str(label) + ' Terms : '+ str([voc[i] for i in topN]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that adding the small summary improves the recognition rate by $10\\%$. \n",
    "\n",
    "As a side note, Naive Bayes with these models creates a linear decision boundary. For this reason, sometimes NB is called a linear classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/7/72/SVM_margin.png/300px-SVM_margin.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machines (SVM) are a powerful method used in machine learning for classification tasks. They belong to a category of algorithms known as discriminative learning, where the goal is to find a decision boundary that separates different classes in the data.\n",
    "\n",
    "#### What Makes SVM Special?\n",
    "\n",
    "Unlike other linear models such as the perceptron or logistic regression, SVM offers a more robust approach to classification. Here's why:\n",
    "\n",
    "- **Explicit Boundary Modeling**: SVM focuses on finding the best possible boundary (or hyperplane in higher dimensions) that separates the classes. This boundary is chosen not just to separate the classes but to do so in a way that maximizes the margin between the closest points of the classes to the boundary. These closest points are known as support vectors, giving the algorithm its name.\n",
    "\n",
    "- **Versatility**: While the classical model for SVM is linear, it can be extended to handle non-linear classification using something called the kernel trick. This allows SVM to classify data that isn't linearly separable by transforming it into a higher-dimensional space where a linear separator does exist.\n",
    "\n",
    "#### The Intuition Behind SVM\n",
    "\n",
    "Imagine you're trying to draw a line that separates apples from oranges on a table. SVM aims to draw this line not just anywhere but in such a way that the smallest distance from the line to the nearest apple or orange is maximized. This ensures that the decision boundary is as far away from the closest points of each class as possible, providing a buffer that helps make the classification more robust to new data points.\n",
    "\n",
    "- **Maximizing the Margin**: The key idea is to find the widest possible \"street\" (margin) between the classes, with the \"edges\" of this street just touching the nearest points of each class. These points at the edge are the support vectors.\n",
    "\n",
    "- **Handling More Complex Data**: For data that can't be separated by a straight line, SVM uses the kernel trick to project the data into a higher-dimensional space where a linear separator is possible. This is like lifting the apples and oranges off the table into the air to find a plane that separates them.\n",
    "\n",
    "#### Conclusion\n",
    "\n",
    "SVM is a foundational tool in machine learning, known for its ability to create clear, well-defined boundaries between classes. Its ability to handle both linear and non-linear data makes it a versatile choice for a wide range of classification problems. Understanding the principles behind SVM allows for deeper insights into how machines can learn to distinguish between different categories of data, making it a crucial part of any machine learning toolkit.\n",
    "\n",
    "\n",
    "[Link SVM](https://www.youtube.com/watch?v=efR1C6CvhmE&ab_channel=StatQuestwithJoshStarmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reset -f\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#from IPython.html.widgets import interact\n",
    "from ipywidgets import interact\n",
    "\n",
    "class HLA():\n",
    "    def __init__(self):\n",
    "        np.random.seed(1)\n",
    "        self.X = np.concatenate([1.25*np.random.randn(40,2),5+1.5*np.random.randn(40,2)]) \n",
    "        self.y = np.concatenate([np.ones((40,1)),-np.ones((40,1))])\n",
    "        plt.scatter(self.X[0:40,0],self.X[0:40,1],color='r')\n",
    "        plt.scatter(self.X[40:,0],self.X[40:,1],color='b') \n",
    "        delta = 0.025\n",
    "        xx = np.arange(-5.0, 10.0, delta)\n",
    "        yy = np.arange(-5.0, 10.0, delta)\n",
    "        XX, YY = np.meshgrid(xx, yy)\n",
    "        Xf = XX.flatten()\n",
    "        Yf = YY.flatten()\n",
    "        self.sz=XX.shape\n",
    "        self.data = np.concatenate([Xf[:,np.newaxis],Yf[:,np.newaxis]],axis=1);\n",
    "\n",
    "    def run(self,w0,w1,offset):\n",
    "        plt.close('all')\n",
    "        w=np.array([w0,w1])\n",
    "        w.shape=(2,1)\n",
    "        Z = self.data.dot(w)+offset\n",
    "        Z.shape=self.sz\n",
    "        plt.scatter(self.X[0:40,0],self.X[0:40,1],color='r')\n",
    "        plt.scatter(self.X[40:,0],self.X[40:,1],color='b')\n",
    "        plt.imshow(Z, interpolation='bilinear', origin='lower', extent=(-5,10,-5,10),alpha=0.3, vmin=-30, vmax=30)\n",
    "        XX = self.data[:,0].reshape(self.sz)\n",
    "        YY = self.data[:,1].reshape(self.sz)\n",
    "        plt.contour(XX,YY,Z,[0])\n",
    "        fig = plt.gcf()\n",
    "        fig.set_size_inches(9,9)\n",
    "   \n",
    "widget_hla = HLA()\n",
    "def decorator(w0,w1,offset):\n",
    "    widget_hla.run(w0,w1,offset)\n",
    "    \n",
    "\n",
    "interact(decorator, w0=(-10.,10.), w1=(-10.,10.), offset=(-20.,40.));\n",
    "# (code replicates the images, due to packages updates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION:** Using the former widget, check manually the following configurations:\n",
    "\n",
    "* ${(w_0, w_1, \\text{offset}) = (-1.7, -3.1, 10)}$\n",
    "* ${(w_0, w_1, \\text{offset}) = (-3.7, -0.5, 10.3)}$\n",
    "* ${(w_0, w_1, \\text{offset}) = (-7.5, -3.2, 28.8)}$\n",
    "\n",
    "Which one of those configurations do you think yields a better boundary? Why?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**INTUITION:** The Support Vector Machine classifer finds the boundary with maximum distance/**margin** to both classes.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "- It implicitly models the notion of noise. One expects that the boundary with maximum margin will be robust to small perturbations in the data.\n",
    "- A maximum margin classifier has a unique solution in the separable case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check the result of fitting a SVM classifier using sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the global namespace in the notebook to start fresh\n",
    "%reset -f\n",
    "\n",
    "# Enable inline plotting for Jupyter notebooks\n",
    "%matplotlib inline\n",
    "\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm  # Import SVM model from scikit-learn\n",
    "\n",
    "# Define a class for the SVM example\n",
    "class svm_example():\n",
    "    def __init__(self):\n",
    "        '''Data creation: Generates synthetic data for classification'''\n",
    "        np.random.seed(1)  # Set seed for reproducibility\n",
    "        # Generate synthetic data: two clusters with normal distribution\n",
    "        self.X = np.concatenate([1.25*np.random.randn(40,2), 5+1.5*np.random.randn(40,2)])\n",
    "        # Generate labels: first 40 are 1, next 40 are -1\n",
    "        self.y = np.concatenate([np.ones((40,1)), -np.ones((40,1))])\n",
    "\n",
    "    def run(self):\n",
    "        '''Fit a linear SVM: Train an SVM classifier with a linear kernel'''\n",
    "        self.clf = svm.SVC(kernel='linear')  # Initialize the SVM with a linear kernel\n",
    "        self.clf.fit(self.X, self.y.ravel())  # Fit the SVM model with the data\n",
    "        \n",
    "    def display(self):\n",
    "        '''Display the decision boundary, margins, and support vectors'''\n",
    "        # Create a mesh grid for plotting decision boundary\n",
    "        delta = 0.25\n",
    "        xx = np.arange(-5.0, 10.0, delta)\n",
    "        yy = np.arange(-5.0, 10.0, delta)\n",
    "        XX, YY = np.meshgrid(xx, yy)\n",
    "        \n",
    "        # Prepare the grid points for prediction\n",
    "        Xf = XX.flatten()\n",
    "        Yf = YY.flatten()\n",
    "        sz = XX.shape\n",
    "        data = np.concatenate([Xf[:, np.newaxis], Yf[:, np.newaxis]], axis=1)\n",
    "        \n",
    "        # Predict the decision function value for each grid point\n",
    "        Z = self.clf.decision_function(data)\n",
    "        Z.shape = sz\n",
    "        \n",
    "        # Plot the data points: red for one class, blue for the other\n",
    "        plt.scatter(self.X[0:40, 0], self.X[0:40, 1], color='r')\n",
    "        plt.scatter(self.X[40:, 0], self.X[40:, 1], color='b')\n",
    "        \n",
    "        # Display the decision boundary and margins\n",
    "        plt.imshow(Z, interpolation='bilinear', origin='lower', extent=(-5,10,-5,10), alpha=0.3, vmin=-3, vmax=3)\n",
    "        plt.contour(XX, YY, Z, [-1, 0, 1], colors=['b', 'k', 'r'])  # Draw the margins and decision boundary\n",
    "        \n",
    "        # Enhance the plot\n",
    "        fig = plt.gcf()\n",
    "        fig.set_size_inches(9, 9)\n",
    "        \n",
    "        # Print the number of support vectors for each class\n",
    "        print('Number of support vectors: ' + str(np.sum(self.clf.n_support_)))\n",
    "        \n",
    "        # Highlight the support vectors on the plot\n",
    "        plt.scatter(self.clf.support_vectors_[:, 0], \n",
    "                    self.clf.support_vectors_[:, 1], \n",
    "                    s=120, \n",
    "                    facecolors='none', \n",
    "                    linewidths=2,\n",
    "                    zorder=10)\n",
    "        \n",
    "        # Print the coefficients of the decision function (w0, w1) and the offset\n",
    "        print('(w0,w1) = ' + str(10*self.clf.coef_[0]))\n",
    "        print('offset = ' + str(10*self.clf.intercept_[0]))\n",
    "        \n",
    "        # Return grid for further use if necessary\n",
    "        return XX, YY, Z\n",
    "\n",
    "# Create an instance of the svm_example class\n",
    "c = svm_example()\n",
    "c.run()  # Train the SVM model\n",
    "XX, YY, Z = c.display()  # Display the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that there is a critical subset of data points. These are called **Support Vectors**. If any of those points disappear the boundary changes.  The decision boundary depends on the support vectors, thus we have to store them in our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the intuition in 3D:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from ipywidgets import interact\n",
    "\n",
    "np.random.seed(1)\n",
    "X = np.concatenate([1.25*np.random.randn(40,2),5+1.5*np.random.randn(40,2)]) \n",
    "y = np.concatenate([np.ones((40,1)),-np.ones((40,1))])\n",
    "def control3D(elevation,azimuth):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    fig.set_size_inches(12,12)\n",
    "    ax.plot_surface(XX,YY,Z,cmap=cm.coolwarm,alpha=0.3,linewidth=0)\n",
    "    ax.scatter(X[0:40,0],X[0:40,1],1,color='r')\n",
    "    ax.scatter(X[40:,0],X[40:,1],-1,color='b')\n",
    "    ax.contour(XX,YY,Z,[-1,0,1],colors=['b','k','r'])\n",
    "    ax.view_init(elev=elevation, azim=azimuth)\n",
    "\n",
    "#Ipython 2.0\n",
    "interact(control3D,elevation=(0.,90.),azimuth=(0,360))\n",
    "#Ipython 1.1\n",
    "#elevation = 45\n",
    "#azimuth = 180\n",
    "#control3D(elevation,azimuth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTIONS:**\n",
    "<li> Set the azimuth to $113$ and elevation to $0$. Observe the data points and the relative position of the hyperplane. \n",
    "<li> Change the elevation to $90$. Describe this projection.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10.2.1 (Theory) Modeling the Support Vector Machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Geometry of the hyperplane\n",
    "A hyperplane in ${\\bf R}^d$ is defined as an affine combination of the variables: $\\pi\\equiv a^Tx + b = 0$. \n",
    "\n",
    "Features:\n",
    "\n",
    "+ A hyperplane splits the space in two half-spaces. The evaluation of the equation of the hyperplane on any element of one of the half-space is a positive value. It is a negative value for all the elements in the other half-space.\n",
    "+ The distance of a point $x \\in{\\bf R}^d$ to the hyperplane $\\pi$ is \n",
    "$$d(x,\\pi)=\\frac{a^Tx+b}{\\|a\\|_2}$$\n",
    "\n",
    "##### Modeling the separating hyperplane\n",
    "Given a binary classification problem with training data $\\mathcal{D}=\\{(x_i,y_i)\\},\\; i=1\\dots N, \\; y_i\\in\\{+1,-1\\} $. Consider $\\mathcal{S} \\subseteq \\mathcal{D}$ the subset of all data points belonging to class $+1$, $\\mathcal{S}=\\{x_i | y_i=+1\\}$, and $\\mathcal{R}=\\{x_i | y_i=-1\\}$ its complement. \n",
    "\n",
    "Then the problem of finding a separating hyperplane consists of fulfilling the following constraints\n",
    "\n",
    "$$a^Ts_i+b>0\\; \\text{and}\\; a^Tr_i+b<0 \\quad \\forall s_i\\in\\mathcal{S}, r_i\\in\\mathcal{R}.$$\n",
    "\n",
    "Note the strict inequalities in the formulation. Informally, we can consider the smallest satisfied constraint. And observe that the rest must be satisfied with a larger value. Thus, we can arbitrarily set that value to 1 and rewrite the problem as $$a^Ts_i+b\\geq 1\\; \\text{and}\\; a^Tr_i+b\\leq -1.$$\n",
    "\n",
    "This is a *feasibility problem* and it is usually written in the following way in optimization standard notation\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{minimize} & 1\\\\\n",
    "\\text{subject to} & a^T r_i + b \\leq -1,\\; \\forall r_i \\in \\mathcal{R}\\\\\n",
    "& a^T s_i + b \\geq 1\\; \\forall s_i \\in \\mathcal{S}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "or in a compact way\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{minimize} & 1\\\\\n",
    "\\text{subject to} & y_i (a^T x_i + b) \\geq 1,\\; \\forall x_i \\in \\mathcal{D}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The solution of this problem is not unique, e.g. remember all the parameters of the 'Human Learning Algorithm'.  \n",
    "\n",
    "##### The maximum margin hyperplane\n",
    "\n",
    "Selecting the maximum margin hyperplane requires to add a new constraint to our problem. Remember from the geometry of the hyperplane that the distance of any point to a hyperplane is given by $d(x,\\pi)=\\frac{a^Tx+b}{\\|a\\|_2}$. \n",
    "\n",
    "Recall that we want positive data to be beyond value 1 and negative data below -1. Thus, what is the distance value we want to maximize?\n",
    "\n",
    "The positive point closest to the boundary is at $1/\\|a\\|_2$ and the negative point closest to the boundary data point is also at $1/\\|a\\|_2$. Thus data points from different classes are at least $2/\\|a\\|_2$ apart. \n",
    "\n",
    "Recall that our goal is to find the separating hyperplane with maximum margin, i.e. with maximum distance among elements from different classes. Thus, we can complete the former formulation with our last requirement as follows\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{maximize} & 2/\\|a\\|_2 \\\\\n",
    "\\text{subject to} & y_i (a^T x_i + b) \\geq 1,\\; \\forall x_i \\in \\mathcal{D}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "or equivalently,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{minimize} & \\|a\\|_2/2 \\\\\n",
    "\\text{subject to} & y_i (a^T x_i + b) \\geq 1,\\; \\forall x_i \\in \\mathcal{D}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "This formulation has a solution as long as the problem is linearly separable.\n",
    "\n",
    "##### Dealing with the non-separable case\n",
    "\n",
    "In order to deal with misclassifications, we are going to introduce a new set of variables $\\xi_i$, that represents the amount of violation in the $i-th$ constraint. If the constraint is already satisfied, then $\\xi_i=0$, and $\\xi_i>0$ otherwise. Because $\\xi_i$ is related to the errors, we would like to keep this amount as close so zero as possible. This makes us introduce a element in the objective trading-off with the maximum margin.\n",
    "\n",
    "The new model becomes\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{minimize} & \\|a\\|_2/2 + C \\sum\\limits_{i=1}^N \\xi_i\\\\\n",
    "\\text{subject to} & y_i (a^T x_i + b) \\geq 1 - \\xi_i,\\; i=1\\dots N\\\\\n",
    "& \\xi_i\\geq 0\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $C$ is the trade-off parameter that roughly balances margin and misclassification rate. This formulation is also called **soft-margin SVM**.\n",
    "\n",
    "**Take home ideas:**\n",
    "<ul>\n",
    "<li> Classical SVM fits a hyperplane separating boundary. </li>\n",
    "<li> The hyperplane is defined to achieve the maximum margin. </li>\n",
    "<li> If the problem is not linearly separable a new term related to the misclassification performance is introduced that trades-off with the margin. This trade-off is governed by parameter $C$ (or $\\nu$ in $\\nu$-SVM). </li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The New York Times problem again\n",
    "\n",
    "Let us now apply our knowledge to the New York Times headlines topic prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ok.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearSVC()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearSVC</label><div class=\"sk-toggleable__content\"><pre>LinearSVC()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearSVC()"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recover NTY data\n",
    "import pickle\n",
    "from scipy import sparse\n",
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "with open('NYT_data.pkl', 'rb') as fname:\n",
    "    data = pickle.load(fname)\n",
    "X_train = data[0]\n",
    "y_train = data[1]\n",
    "X_test = data[2]\n",
    "y_test = data[3]\n",
    "print('Loading ok.')\n",
    "\n",
    "# Ensure target arrays are properly formatted\n",
    "y_train = np.array(y_train).ravel()\n",
    "y_test = np.array(y_test).ravel()\n",
    "\n",
    "# Train the model\n",
    "from sklearn import svm\n",
    "clf = svm.LinearSVC()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification accuracy: 0.5318120805369128\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.65      0.43      0.52       173\n",
      "           2       0.33      0.51      0.40       183\n",
      "           3       0.65      0.65      0.65       440\n",
      "           4       0.20      0.38      0.26        16\n",
      "           5       0.48      0.47      0.47       187\n",
      "           6       0.69      0.64      0.66       212\n",
      "           7       0.38      0.33      0.35        63\n",
      "           8       0.58      0.57      0.58        74\n",
      "          10       0.44      0.42      0.43       132\n",
      "          12       0.47      0.41      0.44       493\n",
      "          13       0.49      0.44      0.46        48\n",
      "          14       0.26      0.42      0.32        90\n",
      "          15       0.26      0.31      0.29       277\n",
      "          16       0.56      0.58      0.57      1290\n",
      "          17       0.38      0.45      0.41       120\n",
      "          18       0.32      0.24      0.28        33\n",
      "          19       0.62      0.62      0.62      1487\n",
      "          20       0.63      0.57      0.60      1047\n",
      "          21       0.29      0.39      0.33        51\n",
      "          24       0.54      0.46      0.50       175\n",
      "          26       0.51      0.56      0.53       174\n",
      "          27       0.14      0.16      0.15        25\n",
      "          28       0.33      0.28      0.30       202\n",
      "          29       0.57      0.44      0.50       297\n",
      "          30       0.64      0.73      0.68        56\n",
      "          31       0.42      0.59      0.49        76\n",
      "          99       0.00      0.00      0.00        29\n",
      "\n",
      "    accuracy                           0.53      7450\n",
      "   macro avg       0.44      0.45      0.44      7450\n",
      "weighted avg       0.54      0.53      0.53      7450\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 900x900 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_hat = clf.predict(X_test)\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_confusion_matrix(y_pred, y):\n",
    "    plt.imshow(metrics.confusion_matrix(y, y_pred), interpolation='nearest')\n",
    "    plt.colorbar()\n",
    "    plt.ylabel('true value')\n",
    "    plt.xlabel('predicted value')\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(9,9)    \n",
    "    \n",
    "print (\"classification accuracy:\", metrics.accuracy_score(y_hat, y_test))\n",
    "plot_confusion_matrix(y_hat, y_test)\n",
    "print (\"Classification Report:\")\n",
    "print (metrics.classification_report(y_hat,np.array(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the default parameters we can improve the recognition rate by $10\\%$. However we can not check the most important words. Can we find a better trade-off?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(estimator=LinearSVC(),\n",
       "             param_grid={&#x27;C&#x27;: [0.01, 0.05, 0.1, 0.5, 1, 10]})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(estimator=LinearSVC(),\n",
       "             param_grid={&#x27;C&#x27;: [0.01, 0.05, 0.1, 0.5, 1, 10]})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: LinearSVC</label><div class=\"sk-toggleable__content\"><pre>LinearSVC()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearSVC</label><div class=\"sk-toggleable__content\"><pre>LinearSVC()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(estimator=LinearSVC(),\n",
       "             param_grid={'C': [0.01, 0.05, 0.1, 0.5, 1, 10]})"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing necessary libraries for model selection\n",
    "from sklearn import model_selection\n",
    "from sklearn import svm\n",
    "\n",
    "# Defining the parameter grid: \n",
    "# 'C' is a regularization parameter for LinearSVC. It controls the trade off between achieving a low training error and a \n",
    "# low testing error (generalization).\n",
    "# A smaller 'C' value leads to a smoother decision boundary (less fitting to the training data), \n",
    "# while a larger 'C' encourages the model to classify all training examples correctly by giving the model more flexibility.\n",
    "# Here, we're defining a range of 'C' values to try out with GridSearchCV to find the best one.\n",
    "parameters = {'C': [0.01, 0.05, 0.1, 0.5, 1, 10]}\n",
    "\n",
    "# Initializing the LinearSVC model\n",
    "svc = svm.LinearSVC()\n",
    "\n",
    "# Setting up GridSearchCV:\n",
    "# 'svc' is the SVM model with a linear kernel to be trained.\n",
    "# 'parameters' contains the grid of parameters ('C' values here) we want to try out.\n",
    "# GridSearchCV will systematically work through the combinations of parameters (different 'C' values),\n",
    "# train the model for each combination, and evaluate its performance.\n",
    "clf = model_selection.GridSearchCV(svc, parameters)\n",
    "\n",
    "# Fitting GridSearchCV:\n",
    "# This will train the LinearSVC model multiple times with the different 'C' values specified in 'parameters'.\n",
    "# For each 'C' value, it uses cross-validation to evaluate the model's performance.\n",
    "# Cross-validation is a technique for assessing how the results of a statistical analysis will generalize to\n",
    "# an independent data set.\n",
    "# It does this by partitioning the original training data set into a training set to train the model,\n",
    "# and a validation set to evaluate it. This process is repeated for each 'C' value.\n",
    "clf.fit(X_train, y_train.ravel())\n",
    "\n",
    "# After .fit() completes, clf (our GridSearchCV object) will contain a lot of information:\n",
    "# - The best 'C' value found.\n",
    "# - The model fitted with the best 'C' value.\n",
    "# - The scores or performance metrics for each 'C' value tried."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameterization is {'C': 0.1}\n",
      "The achieved score is 0.6021027654651043\n",
      "Checking the rest of the scores \n",
      "\n",
      "[0.54049316 0.5961241  0.60210277 0.58391256 0.56919918 0.50915823]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Text(-1.0, 0, '0.01'),\n",
       " Text(0.0, 0, '0.05'),\n",
       " Text(1.0, 0, '0.1'),\n",
       " Text(2.0, 0, '0.5'),\n",
       " Text(3.0, 0, '1'),\n",
       " Text(4.0, 0, '10'),\n",
       " Text(5.0, 0, ''),\n",
       " Text(6.0, 0, '')]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print ('The best parameterization is ' + str(clf.best_params_))\n",
    "print ('The achieved score is ' + str(clf.best_score_))\n",
    "\n",
    "print ('Checking the rest of the scores \\n')\n",
    "import matplotlib.pyplot as plt\n",
    "print(clf.cv_results_['mean_test_score'])\n",
    "\n",
    "plt.plot(clf.cv_results_['mean_test_score'],'r',marker='o')\n",
    "ax = plt.gca()\n",
    "ax.set_xticklabels([0.01, 0.05, 0.1, 0.5, 1, 10])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Best Parametrization and Achieved Score in GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When performing a grid search in machine learning using `GridSearchCV`, the goal is to find the best parameters for a model that result in the highest performance score. Here's what these terms mean:\n",
    "\n",
    "### Best Parametrization\n",
    "\n",
    "- **Best Parametrization**: This refers to the set of parameters that gave the best results after the grid search has been completed. In the context of an SVM with a `C` parameter, it indicates the value of `C` that led to the best model performance. For example, if `GridSearchCV` returns `{'C': 1}`, it means that using `C=1` for the SVM resulted in the most accurate predictions during the cross-validation process.\n",
    "\n",
    "### Achieved Score\n",
    "\n",
    "- **Achieved Score**: After finding the best parametrization, `GridSearchCV` also provides the best score that was achieved with this parameter. This score is a number that reflects how well the model with the best parameters is performing. The specific meaning of the score depends on the scoring method used (e.g., accuracy, precision, recall, F1 score). For instance, if the score is `0.60212765`, and we are using accuracy as our scoring metric, it indicates that the model with the best parameter (`C=1`) was able to correctly predict the class labels for approximately 60.21% of the cross-validated dataset.\n",
    "\n",
    "### Putting It All Together\n",
    "\n",
    "The process of using `GridSearchCV` not only helps in tuning the model to find the best settings but also gives us an estimate of how well the model is likely to perform on unseen data. It's important to look at both the best parameters and the achieved score to understand the potential effectiveness of your model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification accuracy: 0.5675167785234899\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.63      0.50      0.56       141\n",
      "           2       0.31      0.57      0.40       152\n",
      "           3       0.72      0.68      0.70       461\n",
      "           4       0.20      0.43      0.27        14\n",
      "           5       0.49      0.58      0.54       156\n",
      "           6       0.73      0.69      0.71       210\n",
      "           7       0.50      0.58      0.54        48\n",
      "           8       0.67      0.70      0.68        69\n",
      "          10       0.46      0.49      0.47       117\n",
      "          12       0.50      0.42      0.46       527\n",
      "          13       0.44      0.56      0.49        34\n",
      "          14       0.24      0.57      0.34        61\n",
      "          15       0.30      0.40      0.34       245\n",
      "          16       0.61      0.59      0.60      1378\n",
      "          17       0.45      0.54      0.49       118\n",
      "          18       0.28      0.30      0.29        23\n",
      "          19       0.67      0.58      0.62      1712\n",
      "          20       0.69      0.60      0.64      1078\n",
      "          21       0.30      0.55      0.39        38\n",
      "          24       0.54      0.54      0.54       153\n",
      "          26       0.47      0.59      0.52       152\n",
      "          27       0.14      0.67      0.24         6\n",
      "          28       0.34      0.38      0.36       151\n",
      "          29       0.66      0.52      0.58       294\n",
      "          30       0.62      0.71      0.67        56\n",
      "          31       0.36      0.75      0.48        51\n",
      "          99       0.00      0.00      0.00         5\n",
      "\n",
      "    accuracy                           0.57      7450\n",
      "   macro avg       0.46      0.54      0.48      7450\n",
      "weighted avg       0.60      0.57      0.58      7450\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 900x900 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_hat = clf.predict(X_test)\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_confusion_matrix(y_pred, y):\n",
    "    plt.imshow(metrics.confusion_matrix(y, y_pred), interpolation='nearest')\n",
    "    plt.colorbar()\n",
    "    plt.ylabel('true value')\n",
    "    plt.xlabel('predicted value')\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(9,9)    \n",
    "    \n",
    "print (\"classification accuracy:\", metrics.accuracy_score(y_hat, np.array(y_test)))\n",
    "plot_confusion_matrix(y_hat, np.array(y_test))\n",
    "print (\"Classification Report:\")\n",
    "print (metrics.classification_report(y_hat,np.array(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FULL SCOPE EXERCISE (Same as in another Jupyter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cancer.keys(): \n",
      "dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename', 'data_module'])\n",
      "['malignant' 'benign']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0        17.99         10.38          122.80     1001.0          0.11840   \n",
       "1        20.57         17.77          132.90     1326.0          0.08474   \n",
       "2        19.69         21.25          130.00     1203.0          0.10960   \n",
       "3        11.42         20.38           77.58      386.1          0.14250   \n",
       "4        20.29         14.34          135.10     1297.0          0.10030   \n",
       "\n",
       "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0           0.27760          0.3001              0.14710         0.2419   \n",
       "1           0.07864          0.0869              0.07017         0.1812   \n",
       "2           0.15990          0.1974              0.12790         0.2069   \n",
       "3           0.28390          0.2414              0.10520         0.2597   \n",
       "4           0.13280          0.1980              0.10430         0.1809   \n",
       "\n",
       "   mean fractal dimension  ...  worst radius  worst texture  worst perimeter  \\\n",
       "0                 0.07871  ...         25.38          17.33           184.60   \n",
       "1                 0.05667  ...         24.99          23.41           158.80   \n",
       "2                 0.05999  ...         23.57          25.53           152.50   \n",
       "3                 0.09744  ...         14.91          26.50            98.87   \n",
       "4                 0.05883  ...         22.54          16.67           152.20   \n",
       "\n",
       "   worst area  worst smoothness  worst compactness  worst concavity  \\\n",
       "0      2019.0            0.1622             0.6656           0.7119   \n",
       "1      1956.0            0.1238             0.1866           0.2416   \n",
       "2      1709.0            0.1444             0.4245           0.4504   \n",
       "3       567.7            0.2098             0.8663           0.6869   \n",
       "4      1575.0            0.1374             0.2050           0.4000   \n",
       "\n",
       "   worst concave points  worst symmetry  worst fractal dimension  \n",
       "0                0.2654          0.4601                  0.11890  \n",
       "1                0.1860          0.2750                  0.08902  \n",
       "2                0.2430          0.3613                  0.08758  \n",
       "3                0.2575          0.6638                  0.17300  \n",
       "4                0.1625          0.2364                  0.07678  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Supervised Learning Class\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "import pandas as pd\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "print(\"cancer.keys(): \\n{}\".format(cancer.keys()))\n",
    "print(cancer['target_names'])\n",
    "\n",
    "data = pd.DataFrame(cancer['data'], columns = cancer['feature_names'])\n",
    "target = pd.DataFrame(cancer['target'], columns = ['target'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter and choices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Selection?\n",
    "# Non-transformed vs Standardization?\n",
    "# Tune the parameters of the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split in Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer['data'], cancer['target'], random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mean texture',\n",
       " 'mean smoothness',\n",
       " 'mean symmetry',\n",
       " 'mean fractal dimension',\n",
       " 'texture error',\n",
       " 'smoothness error',\n",
       " 'compactness error',\n",
       " 'concavity error',\n",
       " 'concave points error',\n",
       " 'symmetry error',\n",
       " 'fractal dimension error',\n",
       " 'worst texture',\n",
       " 'worst smoothness',\n",
       " 'worst symmetry',\n",
       " 'worst fractal dimension']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature Selection\n",
    "import seaborn as sns\n",
    "#display(pd.DataFrame(X_train, columns = data.columns).corr())\n",
    "\n",
    "features = pd.DataFrame(X_train, columns = data.columns)\n",
    "target = pd.Series(y_train)\n",
    "corr_dict = {}\n",
    "cols_to_remove = []\n",
    "\n",
    "for col in list(features.columns):\n",
    "  corr_dict[col] = abs(features[col].corr(target))\n",
    "  if corr_dict[col] < 0.5:\n",
    "    cols_to_remove.append(col)\n",
    "\n",
    "#corr_dict\n",
    "cols_to_remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create \"Feature Selected sets\"\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer['data'], \n",
    "                                                    cancer['target'], \n",
    "                                                    random_state=0)\n",
    "\n",
    "c_data = pd.DataFrame(cancer['data'], columns = cancer['feature_names'])\n",
    "c_data = c_data.drop(cols_to_remove, axis=1)\n",
    "\n",
    "X_train_featsel, X_test_featsel, y_train, y_test = train_test_split(c_data, \n",
    "                                                                    cancer['target'],\n",
    "                                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardiztion\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "##### X_train #########\n",
    "# create standardization object\n",
    "scale = StandardScaler().fit(features) # \"features\" is just the training data\n",
    "\n",
    "X_train_standard = scale.transform(X_train)\n",
    "X_test_standard  = scale.transform(X_test)          \n",
    "\n",
    "X_train_standard = pd.DataFrame(X_train_standard, columns = features.columns)\n",
    "X_test_standard = pd.DataFrame(X_test_standard, columns = features.columns)\n",
    "\n",
    "##### X_train_featsel #########\n",
    "# create standardization object\n",
    "scale = StandardScaler().fit(features) # \"features\" is just the training data\n",
    "\n",
    "X_train_featsel_standard = scale.transform(X_train)\n",
    "X_test_featsel_standard = scale.transform(X_test)\n",
    "\n",
    "X_train_featsel_standard = pd.DataFrame(X_train_featsel_standard, columns = features.columns)\n",
    "X_test_featsel_standard = pd.DataFrame(X_test_featsel_standard, columns = features.columns)\n",
    "\n",
    "\n",
    "# Standardization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "##### X_train #########\n",
    "# create standardization object\n",
    "scale = StandardScaler().fit(features)\n",
    "\n",
    "X_train_standard = scale.transform(X_train)\n",
    "X_train_standard = pd.DataFrame(X_train_standard, columns = features.columns)\n",
    "\n",
    "X_test_standard  = scale.transform(X_test)\n",
    "X_test_standard = pd.DataFrame(X_test_standard, columns = features.columns)\n",
    "\n",
    "##### X_train_featsel #########\n",
    "# create standardization object\n",
    "features_featsel = pd.DataFrame(X_train_featsel, columns = c_data.columns)\n",
    "scale = StandardScaler().fit(features_featsel)\n",
    "\n",
    "X_train_featsel_standard = scale.transform(X_train_featsel)\n",
    "X_train_featsel_standard = pd.DataFrame(X_train_featsel_standard, columns = c_data.columns)\n",
    "\n",
    "X_test_featsel_standard = scale.transform(X_test_featsel)\n",
    "X_test_featsel_standard = pd.DataFrame(X_test_featsel_standard, columns = c_data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normal\n",
      "featsel\n",
      "standard\n",
      "featsel_standard\n",
      "0.9790143964562569\n",
      "0.8593576965669989\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['normal', 'KNN', '1', 0.9179955703211518],\n",
       " ['normal', 'KNN', '3', 0.9180509413067552],\n",
       " ['normal', 'KNN', '5', 0.92718715393134],\n",
       " ['normal', 'KNN', '7', 0.9272978959025471],\n",
       " ['normal', 'KNN', '9', 0.9272425249169436],\n",
       " ['normal', 'SVM', '0.001', 0.9109634551495016],\n",
       " ['normal', 'SVM', '0.01', 0.920265780730897],\n",
       " ['normal', 'SVM', '0.1', 0.906312292358804],\n",
       " ['normal', 'SVM', '1', 0.8689368770764121],\n",
       " ['normal', 'SVM', '10', 0.9131229235880399],\n",
       " ['featsel', 'KNN', '1', 0.9084717607973423],\n",
       " ['featsel', 'KNN', '3', 0.9109634551495016],\n",
       " ['featsel', 'KNN', '5', 0.9224806201550388],\n",
       " ['featsel', 'KNN', '7', 0.929623477297896],\n",
       " ['featsel', 'KNN', '9', 0.9272425249169436],\n",
       " ['featsel', 'SVM', '0.001', 0.9108527131782946],\n",
       " ['featsel', 'SVM', '0.01', 0.887264673311185],\n",
       " ['featsel', 'SVM', '0.1', 0.8593576965669989],\n",
       " ['featsel', 'SVM', '1', 0.910797342192691],\n",
       " ['featsel', 'SVM', '10', 0.8726467331118494],\n",
       " ['standard', 'KNN', '1', nan],\n",
       " ['standard', 'KNN', '3', nan],\n",
       " ['standard', 'KNN', '5', nan],\n",
       " ['standard', 'KNN', '7', nan],\n",
       " ['standard', 'KNN', '9', nan],\n",
       " ['standard', 'SVM', '0.001', 0.9719269102990034],\n",
       " ['standard', 'SVM', '0.01', 0.9766334440753045],\n",
       " ['standard', 'SVM', '0.1', 0.9790143964562569],\n",
       " ['standard', 'SVM', '1', 0.9673864894795127],\n",
       " ['standard', 'SVM', '10', 0.9626799557032115],\n",
       " ['featsel_standard', 'KNN', '1', 0.9178848283499447],\n",
       " ['featsel_standard', 'KNN', '3', 0.9319490586932447],\n",
       " ['featsel_standard', 'KNN', '5', 0.9413621262458471],\n",
       " ['featsel_standard', 'KNN', '7', 0.9391472868217055],\n",
       " ['featsel_standard', 'KNN', '9', 0.9344407530454042],\n",
       " ['featsel_standard', 'SVM', '0.001', 0.9344407530454042],\n",
       " ['featsel_standard', 'SVM', '0.01', 0.9344407530454042],\n",
       " ['featsel_standard', 'SVM', '0.1', 0.9483388704318937],\n",
       " ['featsel_standard', 'SVM', '1', 0.953156146179402],\n",
       " ['featsel_standard', 'SVM', '10', 0.9484496124031008]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "the best model is:  ['standard', 'SVM', '0.1', 0.9790143964562569]\n"
     ]
    }
   ],
   "source": [
    "# Model Selection\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def model_picker(model_name, parameter):\n",
    "  if model_name == 'KNN':\n",
    "    return KNeighborsClassifier(n_neighbors = parameter)\n",
    "  if model_name == 'SVM':\n",
    "    return LinearSVC(C = parameter)\n",
    "  else:\n",
    "    raise ValueError(\"I dont know this model\")\n",
    "\n",
    "model_selection = []\n",
    "cv_acc_list = []\n",
    "\n",
    "train_data_dict = {'normal':X_train, \n",
    "              'featsel':X_train_featsel,\n",
    "              'standard': X_train_standard,\n",
    "              'featsel_standard': X_train_featsel_standard}\n",
    "\n",
    "model_list = ['KNN', 'SVM']\n",
    "model_para = {'KNN':[1,3,5,7,9], 'SVM':[0.001, 0.01, 0.1, 1, 10]}\n",
    "\n",
    "for train_data_type in ['normal', 'featsel', 'standard', 'featsel_standard']:\n",
    "  train_data = train_data_dict[train_data_type]\n",
    "  print(train_data_type)\n",
    "  for model_name in model_list:\n",
    "    for p in model_para[model_name]:\n",
    "      #print(p)\n",
    "      model = model_picker(model_name, p) \n",
    "      model.fit(train_data, y_train)\n",
    "\n",
    "      cv_acc = cross_validate(model,train_data, y_train, cv = 10)['test_score'].mean()\n",
    "      #print(train_data_type, model_name, p, cv_acc)\n",
    "      model_selection.append([train_data_type, model_name, str(p), cv_acc])\n",
    "      cv_acc_list.append(cv_acc)\n",
    "\n",
    "print(max(cv_acc_list))\n",
    "print(min(cv_acc_list))\n",
    "\n",
    "print('\\n')\n",
    "display(model_selection)\n",
    "print('\\n')\n",
    "for best in model_selection:\n",
    "  #print(best)\n",
    "  if best[3] == max(cv_acc_list):\n",
    "    print('the best model is: ', best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.972027972027972"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Test your model\n",
    "model = model_picker('SVM', 0.01)\n",
    "model.fit(X_train_standard, y_train)\n",
    "model.score(X_test_standard, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do I \"share\"/use what I just did?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cancer.keys(): \n",
      "dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename', 'data_module'])\n",
      "['malignant' 'benign']\n"
     ]
    }
   ],
   "source": [
    "# Supervised Learning Class\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "import pandas as pd\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "print(\"cancer.keys(): \\n{}\".format(cancer.keys()))\n",
    "print(cancer['target_names'])\n",
    "\n",
    "data = pd.DataFrame(cancer['data'], columns = cancer['feature_names'])\n",
    "target = pd.DataFrame(cancer['target'], columns = ['target'])\n",
    "data.head()\n",
    "\n",
    "cols_to_remove = ['mean texture', \n",
    "                'mean smoothness', \n",
    "                'mean symmetry', \n",
    "                'mean fractal dimension', \n",
    "                'texture error', \n",
    "                'smoothness error', \n",
    "                'compactness error', \n",
    "                'concavity error', \n",
    "                'concave points error', \n",
    "                'symmetry error', \n",
    "                'fractal dimension error', \n",
    "                'worst texture', \n",
    "                'worst smoothness', \n",
    "                'worst symmetry', \n",
    "                'worst fractal dimension']\n",
    "\n",
    "def model_picker(model_name, parameter):\n",
    "  if model_name == 'KNN':\n",
    "    return KNeighborsClassifier(n_neighbors = parameter)\n",
    "  if model_name == 'SVM':\n",
    "    return LinearSVC(C = parameter)\n",
    "  else:\n",
    "    raise ValueError(\"I dont know this model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The columns to remove are ['mean texture', 'mean smoothness', 'mean symmetry', 'mean fractal dimension', 'texture error', 'smoothness error', 'compactness error', 'concavity error', 'concave points error', 'symmetry error', 'fractal dimension error', 'worst texture', 'worst smoothness', 'worst symmetry', 'worst fractal dimension']\n",
      "The Standardizer we should use is: StandardScaler()\n",
      "The model we should use is:  LinearSVC(C=0.01)\n"
     ]
    }
   ],
   "source": [
    "# We need to:\n",
    "features = pd.DataFrame(cancer['data'], columns = data.columns)\n",
    "target = pd.Series(cancer['target'])\n",
    "\n",
    "# Save the preprocessing steps (feature selection and standardization):\n",
    "print(f'The columns to remove are {cols_to_remove}')\n",
    "\n",
    "# Drop the un-selected columns\n",
    "features = features.drop(cols_to_remove, axis=1)\n",
    "\n",
    "# Rebuild the Scaler with all the data\n",
    "scale = StandardScaler().fit(features)\n",
    "print(f'The Standardizer we should use is: {scale}')\n",
    "\n",
    "# Save the model (save the trained model with adjusted internal parameters in a file)\n",
    "#model = model_picker('SVM', 0.01)\n",
    "model = LinearSVC(C = 0.01)\n",
    "\n",
    "features = pd.DataFrame(scale.transform(features) , columns = features.columns) # Normalize the features before training the model\n",
    "model.fit(features, target)\n",
    "print('The model we should use is: ', model)\n",
    "# When using this we need to load the aforementioned elements\n",
    "\n",
    "######################################\n",
    "## SAVE THIS FUNCTION IN A .py FILE  or do this with another student ##\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def pre_processing(features):\n",
    "\n",
    "  # Feature Selection\n",
    "  cols_to_remove = ['mean texture', \n",
    "                  'mean smoothness', \n",
    "                  'mean symmetry', \n",
    "                  'mean fractal dimension', \n",
    "                  'texture error', \n",
    "                  'smoothness error', \n",
    "                  'compactness error', \n",
    "                  'concavity error', \n",
    "                  'concave points error', \n",
    "                  'symmetry error', \n",
    "                  'fractal dimension error', \n",
    "                  'worst texture', \n",
    "                  'worst smoothness', \n",
    "                  'worst symmetry', \n",
    "                  'worst fractal dimension']\n",
    "  \n",
    "  features = features.drop(cols_to_remove, axis=1)\n",
    "\n",
    "  # Feature Scaling\n",
    "  scale = pickle.load(open('Scaler_for_cancer.sav', 'rb'))\n",
    "  return pd.DataFrame(scale.transform(features) , columns = features.columns)\n",
    "\n",
    "######################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# save the Standardizer to disk\n",
    "filename = 'Scaler_for_cancer.sav'\n",
    "pickle.dump(scale, open(filename, 'wb'))\n",
    "\n",
    "# save the model to disk\n",
    "filename = 'Model_for_cancer.sav'\n",
    "pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And with this, you have all the elements you need to run this again: the model, the scaler and the preprocessing code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_ironhack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
